{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import einops\n",
    "import transformer_lens\n",
    "from functools import partial\n",
    "import sae_vis\n",
    "from IPython.display import HTML, display\n",
    "import jaxtyping as jt\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "# import jax # just for tree map whcih i deleted\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2xl = transformer_lens.HookedSAETransformer.from_pretrained_no_processing(\"gpt2-xl\")\n",
    "# SAE was trained without TL's nice things.\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This SAE was trained without TL centering and bias removal. Make sure to usetransformer_lens.HookedSAETransformer.from_pretrained_no_processing('gpt2-xl')\n"
     ]
    }
   ],
   "source": [
    "sae = transformer_lens.HookedSAE.from_pretrained(\"gpt2-xl-saex-resid-pre-l20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_dataset(\"Elriggs/openwebtext-100k\", streaming=False)\n",
    "data = data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30cd10781847bfb27337d0676c8a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "total_sae_loss = 0.0\n",
    "total_toks = 0\n",
    "\n",
    "for _ in tqdm(range(200)):\n",
    "    text = next(iter_data)[\"text\"]\n",
    "    tokens = gpt2xl.to_tokens(text)[:, :128]\n",
    "    total_toks += tokens[:, :-1].numel()\n",
    "    site = \"blocks.20.hook_resid_pre\"\n",
    "    logits, all_acts = gpt2xl.run_with_cache(\n",
    "        tokens,\n",
    "        names_filter = site,\n",
    "    )\n",
    "    acts = all_acts[site]\n",
    "    def get_neglogprobs(logits, tokens):\n",
    "        neglogprobs = -logits.log_softmax(dim=-1)[\n",
    "            torch.arange(logits.shape[0])[:, None],\n",
    "            torch.arange(logits.shape[1]-1)[None],\n",
    "            tokens[:, 1:]\n",
    "        ]\n",
    "        return neglogprobs\n",
    "    neglogprobs = get_neglogprobs(logits, tokens)\n",
    "    total_loss += neglogprobs.sum().item()\n",
    "\n",
    "    sae_logits = gpt2xl.run_with_saes(\n",
    "        tokens,\n",
    "        saes = [sae],\n",
    "    )\n",
    "    \n",
    "    sae_loss = get_neglogprobs(sae_logits, tokens)\n",
    "    total_sae_loss += sae_loss.sum().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.7160975671002245, 2.9482859949427325)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_loss / total_toks, total_sae_loss / total_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.23 loss diff is much bigger than claimed... let's press on anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 51200])\n"
     ]
    }
   ],
   "source": [
    "sae_acts_pre_hook_name = \"blocks.20.hook_resid_pre.hook_sae_acts_pre\"\n",
    "sae_logits, sae_cache = gpt2xl.run_with_cache_with_saes(\n",
    "    tokens,\n",
    "    saes = [sae],\n",
    "    names_filter=sae_acts_pre_hook_name\n",
    ")\n",
    "sae_acts_pre = sae_cache[sae_acts_pre_hook_name]\n",
    "print(sae_acts_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.7578, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print avg L0\n",
    "\n",
    "((sae_acts_pre > 0).sum(dim=-1).float()).mean()\n",
    "\n",
    "# 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbf511eb6ff47bfbfc6424cd1bb3447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=10):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59374 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56534 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (55671 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (58948 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59253 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (74379 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (61556 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59558 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (68363 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (76998 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([888650, 128])\n"
     ]
    }
   ],
   "source": [
    "# from SAE vis demo.\n",
    "SEQ_LEN = 128\n",
    "\n",
    "# Tokenize the data (using a utils function) and shuffle it\n",
    "tokenized_data = transformer_lens.utils.tokenize_and_concatenate(data, gpt2xl.tokenizer, max_length=SEQ_LEN) # type: ignore\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "\n",
    "# Get the tokens as a tensor\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "assert isinstance(all_tokens, torch.Tensor)\n",
    "\n",
    "print(all_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_vis_sae_cfg = sae_vis.model_fns.AutoEncoderConfig(\n",
    "    d_in=sae.cfg.d_in,\n",
    "    d_hidden=sae.cfg.d_sae\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ugh gross TODO(conmy): raise issues to standardise the SAEs used by various libraries\n",
    "\n",
    "sae_vis_sae = sae_vis.model_fns.AutoEncoder(sae_vis_sae_cfg)\n",
    "sae_vis_sae.load_state_dict(sae.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeccb7a926b4751b0be41e5b49dabfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd952a03d56e4b9cb7ba7e67c010b53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sae_vis\u001b[38;5;241m.\u001b[39mSaeVisData\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     12\u001b[0m         encoder \u001b[38;5;241m=\u001b[39m sae_vis_sae,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# encoder_B = encoder_B,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         cfg \u001b[38;5;241m=\u001b[39m sae_vis_config,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m feature_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m126\u001b[39m\n\u001b[0;32m---> 20\u001b[0m sae_vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_sae_vis_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mget_sae_vis_data\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m      3\u001b[0m sae_vis_config \u001b[38;5;241m=\u001b[39m sae_vis\u001b[38;5;241m.\u001b[39mSaeVisConfig(\n\u001b[1;32m      4\u001b[0m     hook_point \u001b[38;5;241m=\u001b[39m site,\n\u001b[1;32m      5\u001b[0m     features \u001b[38;5;241m=\u001b[39m features,\n\u001b[1;32m      6\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m      7\u001b[0m     verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Gather the feature data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msae_vis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSaeVisData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msae_vis_sae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# encoder_B = encoder_B,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgpt2xl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mall_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msae_vis_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/sae_vis/data_storing_fns.py:1037\u001b[0m, in \u001b[0;36mSaeVisData.create\u001b[0;34m(cls, encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     encoder_wrapper \u001b[38;5;241m=\u001b[39m encoder\n\u001b[0;32m-> 1037\u001b[0m sae_vis_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_feature_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_B\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_B\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m   1045\u001b[0m sae_vis_data\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/sae_vis/data_fetching_fns.py:617\u001b[0m, in \u001b[0;36mget_feature_data\u001b[0;34m(encoder, model, tokens, cfg, encoder_B)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# For each batch of features: get new data and update global data storage objects\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m feature_batches:\n\u001b[0;32m--> 617\u001b[0m     new_feature_data, new_time_logs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     sae_vis_data\u001b[38;5;241m.\u001b[39mupdate(new_feature_data)\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m new_time_logs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/sae_vis/data_fetching_fns.py:491\u001b[0m, in \u001b[0;36m_get_feature_data\u001b[0;34m(encoder, encoder_B, model, tokens, feature_indices, cfg, progress)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m token_minibatches:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# Fwd pass, get model activations\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 491\u001b[0m     residual, model_acts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     time_logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(2) Forward passes to gather model activations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# Compute feature activations from this\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/sae_vis/model_fns.py:196\u001b[0m, in \u001b[0;36mTransformerLensWrapper.forward\u001b[0;34m(self, tokens, return_logits)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    tokens: Int[Tensor, \"batch seq\"]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m        If False, returns (residual, activation)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Run with hook functions to store the activations & final value of residual stream\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# If return_logits is False, then we compute the last residual stream value but not the logits\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m output: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# stop_at_layer = (None if return_logits else self.hook_layer),\u001b[39;49;00m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_fn_store_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_point_resid_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_fn_store_act\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# The hook functions work by storing data in model's hook context, so we pop them back out\u001b[39;00m\n\u001b[1;32m    206\u001b[0m activation: Tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhook_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_point]\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/hook_points.py:358\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m     )\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:522\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    515\u001b[0m ):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         (\n\u001b[1;32m    518\u001b[0m             residual,\n\u001b[1;32m    519\u001b[0m             tokens,\n\u001b[1;32m    520\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    521\u001b[0m             attention_mask,\n\u001b[0;32m--> 522\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:335\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    332\u001b[0m     pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_pos_embed(\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(tokens, pos_offset, attention_mask)\n\u001b[1;32m    334\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpos_embed\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     shortformer_pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# If we're using shortformer style attention, we don't add the positional embedding to\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;66;03m# the residual stream. See HookedTransformerConfig for details\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "def get_sae_vis_data(features):\n",
    "    # Specify the hook point you're using, the features you're analyzing, and the batch size for gathering activations\n",
    "    sae_vis_config = sae_vis.SaeVisConfig(\n",
    "        hook_point = site,\n",
    "        features = features,\n",
    "        batch_size = 2048,\n",
    "        verbose = True,\n",
    "    )\n",
    "\n",
    "    # Gather the feature data\n",
    "    return sae_vis.SaeVisData.create(\n",
    "        encoder = sae_vis_sae,\n",
    "        # encoder_B = encoder_B,\n",
    "        model = gpt2xl,\n",
    "        tokens = all_tokens, # type: ignore\n",
    "        cfg = sae_vis_config,\n",
    "    )\n",
    "\n",
    "feature_idx = 126\n",
    "sae_vis_data = get_sae_vis_data([feature_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving feature-centric vis: 100%|██████████| 1/1 [00:00<00:00, 14.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def save_sae_vis_html(sae_vis_data):\n",
    "    # Save as HTML file & open in browser (or not, if in Colab)\n",
    "    filenames = []\n",
    "    for feature_idx in sae_vis_data.feature_data_dict.keys():\n",
    "        filename = f\"feature_vis_demo_{int(1000*time.time())}.html\"\n",
    "        sae_vis_data.save_feature_centric_vis(filename, feature_idx=feature_idx)\n",
    "        filenames.append(filename)\n",
    "    return filenames\n",
    "\n",
    "filename = save_sae_vis_html(sae_vis_data)[0]  # Download to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2xl.W_E.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72ddae61cef4a77a6fe00883a4e8c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 74\u001b[0m\n\u001b[1;32m     64\u001b[0m         output \u001b[38;5;241m=\u001b[39m gpt2xl\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     65\u001b[0m             tokens,\n\u001b[1;32m     66\u001b[0m             max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Params in Turner blog post\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m             return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpt2xl\u001b[38;5;241m.\u001b[39mto_string(output)\n\u001b[0;32m---> 74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mget_steered_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manger_steering_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 64\u001b[0m, in \u001b[0;36mget_steered_completion\u001b[0;34m(tokens, steering_vec, indices)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_steered_completion\u001b[39m(\n\u001b[1;32m     55\u001b[0m     tokens,\n\u001b[1;32m     56\u001b[0m     steering_vec,\n\u001b[1;32m     57\u001b[0m     indices,\n\u001b[1;32m     58\u001b[0m ):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gpt2xl\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m     60\u001b[0m         fwd_hooks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     61\u001b[0m             (site, partial(activation_generation_hook, v\u001b[38;5;241m=\u001b[39msteering_vec, indices\u001b[38;5;241m=\u001b[39mindices))\n\u001b[1;32m     62\u001b[0m         ]\n\u001b[1;32m     63\u001b[0m     ):\n\u001b[0;32m---> 64\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2xl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Params in Turner blog post\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfreq_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gpt2xl\u001b[38;5;241m.\u001b[39mto_string(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:2123\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[0;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, use_past_kv_cache, prepend_bos, padding_side, return_type, verbose)\u001b[0m\n\u001b[1;32m   2120\u001b[0m final_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[0;32m-> 2123\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfinal_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreq_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg))\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2132\u001b[0m     sampled_tokens \u001b[38;5;241m=\u001b[39m final_logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   2133\u001b[0m         devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m   2134\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/utils.py:392\u001b[0m, in \u001b[0;36msample_logits\u001b[0;34m(final_logits, top_k, top_p, temperature, freq_penalty, tokens)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m top_p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m top_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p has to be in (0, 1]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 392\u001b[0m     sorted_logits, sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     cumulative_probs \u001b[38;5;241m=\u001b[39m sorted_logits\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# We round up - we want prob >= top_p not <top_p\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Using the AF post\n",
    "\n",
    "pos_prompt = \"Anger\"  # @param {\"type\": \"string\"}\n",
    "neg_prompt = \"Calm\"  # @param {\"type\": \"string\"}\n",
    "\n",
    "pos_tokens = gpt2xl.to_tokens(pos_prompt, prepend_bos=True)\n",
    "neg_tokens = gpt2xl.to_tokens(neg_prompt, prepend_bos=True)\n",
    "assert pos_tokens.shape == neg_tokens.shape, (pos_tokens.shape, \"!=\" , neg_tokens.shape)\n",
    "\n",
    "gpt2xl.reset_hooks()\n",
    "_, cache = gpt2xl.run_with_cache(\n",
    "    pos_tokens,\n",
    "    names_filter = site,\n",
    ")\n",
    "pos_vec = cache[site][0, :]\n",
    "\n",
    "gpt2xl.reset_hooks()\n",
    "_, neg_cache = gpt2xl.run_with_cache(\n",
    "    neg_tokens,\n",
    "    names_filter = site,\n",
    ")\n",
    "\n",
    "neg_vec = neg_cache[site][0, :]\n",
    "anger_steering_vec = 20*(pos_vec - neg_vec)\n",
    "\n",
    "def activation_generation_hook(\n",
    "    clean_activation: jt.Float[torch.Tensor, \"Batch Seq *Dim\"],\n",
    "    hook: Any,\n",
    "    indices: slice,\n",
    "    v: jt.Float[torch.Tensor, \"SubSeq *Dim\"],\n",
    "    debug: bool = False,\n",
    ") -> jt.Float[torch.Tensor, \"Batch Seq Dim\"]:\n",
    "  \"\"\"TransformerLens hook only impacting prompt not rollout.\"\"\"\n",
    "\n",
    "  if clean_activation.shape[1] == 1:\n",
    "    # Doing autoregression. No injection\n",
    "    return clean_activation\n",
    "\n",
    "  if debug:\n",
    "    print(\"NORM PRE ADD\", clean_activation[:, indices].norm(dim=-1))\n",
    "\n",
    "  clean_activation[:, indices] += v\n",
    "\n",
    "  if debug:\n",
    "    print(\"NORM POST ADD\", clean_activation[:, indices].norm(dim=-1))\n",
    "\n",
    "  return clean_activation\n",
    "\n",
    "prompt = \"I think you're\"\n",
    "torch.random.manual_seed(100)\n",
    "\n",
    "tokens = gpt2xl.to_tokens(prompt)\n",
    "\n",
    "def get_steered_completion(\n",
    "    tokens,\n",
    "    steering_vec,\n",
    "    indices,\n",
    "):\n",
    "    with gpt2xl.hooks(\n",
    "        fwd_hooks=[\n",
    "            (site, partial(activation_generation_hook, v=steering_vec, indices=indices))\n",
    "        ]\n",
    "    ):\n",
    "        output = gpt2xl.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=30,  # Params in Turner blog post\n",
    "            top_p=0.3,\n",
    "            temperature=1.0,\n",
    "            freq_penalty=1.0,\n",
    "            return_type=\"tensor\"\n",
    "        )\n",
    "    return gpt2xl.to_string(output)\n",
    "\n",
    "output = get_steered_completion(tokens, steering_vec=anger_steering_vec, indices=slice(0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|endoftext|>I think you're a bad person. You should be shot, killed, or some other form of humiliation for what you did to me. I am not responsible for the\"]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_angry(prompt: str, verbose: bool=False):\n",
    "    anger_words = [\"kill\", \"anger\"]  # There's a bigger list in our AF post not here.\n",
    "    for word in anger_words:\n",
    "      if word.lower() in prompt.lower():\n",
    "        if verbose:\n",
    "            print(word)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_angry(output[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_tokens = einops.repeat(\n",
    "   tokens,\n",
    "   \"1 Seq -> Batch Seq\",\n",
    "   Batch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.69it/s]\n"
     ]
    }
   ],
   "source": [
    "many_outputs = get_steered_completion(\n",
    "   many_tokens,\n",
    "   steering_vec=anger_steering_vec,\n",
    "   indices = slice(0, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_angry = [is_angry(prompt, False) for prompt in many_outputs]\n",
    "many_angry.count(True) / len(many_angry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry_acts = gpt2xl.run_with_cache_with_saes(\n",
    "   \"Anger\",\n",
    "   names_filter=sae_acts_pre_hook_name,\n",
    "   saes = [sae],\n",
    ")[1][sae_acts_pre_hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_firing_features = sorted(\n",
    "   enumerate(angry_acts[0, -1].tolist()), key=lambda x:-x[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_firing_features = [x for x, _ in most_firing_features[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we going to do now?\n",
    "#\n",
    "# Are there several anger features (cool).\n",
    "#\n",
    "# Could also look into the Eiffel Tower is in Rome steering vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prompt = \"The Eiffel Tower is in Rome\"  # @param {\"type\": \"string\"}\n",
    "neg_prompt = \"The Eiffel Tower is in France\"  # @param {\"type\": \"string\"}\n",
    "\n",
    "pos_tokens = gpt2xl.to_tokens(pos_prompt, prepend_bos=True)\n",
    "neg_tokens = gpt2xl.to_tokens(neg_prompt, prepend_bos=True)\n",
    "assert pos_tokens.shape == neg_tokens.shape, (pos_tokens.shape, \"!=\" , neg_tokens.shape)\n",
    "\n",
    "gpt2xl.reset_hooks()\n",
    "_, cache = gpt2xl.run_with_cache(\n",
    "    pos_tokens,\n",
    "    names_filter = site,\n",
    ")\n",
    "pos_vec = cache[site][0, :]\n",
    "\n",
    "gpt2xl.reset_hooks()\n",
    "_, neg_cache = gpt2xl.run_with_cache(\n",
    "    neg_tokens,\n",
    "    names_filter = site,\n",
    ")\n",
    "\n",
    "neg_vec = neg_cache[site][0, :]\n",
    "rome_steering_vec = (20*(pos_vec - neg_vec))[-1:, :]\n",
    "# I hope this works\n",
    "\n",
    "tokens = \"To see the eiffel tower, people flock to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|endoftext|>To see the eiffel tower, people flock to France's most famous landmark.\\n\\nRome's historic center is filled with statues and monuments, but one of the most iconic images of France's\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = get_steered_completion(tokens, steering_vec=rome_steering_vec, indices=slice(pos_tokens.shape[-1]-1, pos_tokens.shape[-1]))\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis:  19%|█▉        | 6/32 [00:10<00:47,  1.83s/it]"
     ]
    }
   ],
   "source": [
    "top20_sae_vis_data = get_sae_vis_data(top20_firing_features)\n",
    "top20_filenames = save_sae_vis_html(top20_sae_vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rome_sae_logits, rome_sae_cache = gpt2xl.run_with_cache_with_saes(\n",
    "    \"The Eiffel Tower is in Rome\",\n",
    "    saes = [sae],\n",
    "    names_filter=sae_acts_pre_hook_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rome_sae_acts = rome_sae_cache[sae_acts_pre_hook_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_firing_features = sorted(\n",
    "   enumerate(rome_sae_acts[0, -1].tolist()), key=lambda x:-x[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_firing_features = [x for x, _ in most_firing_features[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 32/32 [00:58<00:00,  1.84s/it]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task                                           </span>┃<span style=\"font-weight: bold\"> Time   </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s  │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 40.21s │ 66.6% │\n",
       "│ (3) Computing feature acts from model acts     │ 18.14s │ 30.1% │\n",
       "│ (4) Getting data for tables                    │ 0.00s  │ 0.0%  │\n",
       "│ (5) Getting data for histograms                │ 1.09s  │ 1.8%  │\n",
       "│ (6) Getting data for sequences                 │ 0.90s  │ 1.5%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.03s  │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask                                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│ (1) Initialization                             │ 0.00s  │ 0.0%  │\n",
       "│ (2) Forward passes to gather model activations │ 40.21s │ 66.6% │\n",
       "│ (3) Computing feature acts from model acts     │ 18.14s │ 30.1% │\n",
       "│ (4) Getting data for tables                    │ 0.00s  │ 0.0%  │\n",
       "│ (5) Getting data for histograms                │ 1.09s  │ 1.8%  │\n",
       "│ (6) Getting data for sequences                 │ 0.90s  │ 1.5%  │\n",
       "│ (7) Getting data for quantiles                 │ 0.03s  │ 0.0%  │\n",
       "└────────────────────────────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward passes to cache data for vis: 100%|██████████| 32/32 [01:00<00:00,  1.89s/it]\n",
      "Extracting vis data from cached data: 100%|██████████| 20/20 [01:00<00:00,  3.02s/it]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.57it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.58it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.71it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 13.16it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.43it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.62it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.54it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 12.86it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.62it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.55it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.80it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 13.02it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.69it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.67it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.79it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.66it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 13.15it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.84it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.77it/s]\n",
      "Saving feature-centric vis: 100%|██████████| 20/20 [00:01<00:00, 15.62it/s]\n"
     ]
    }
   ],
   "source": [
    "rome_top20_sae_vis_data = get_sae_vis_data(top20_firing_features)\n",
    "top20_filenames = save_sae_vis_html(rome_top20_sae_vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44460,\n",
       " 30211,\n",
       " 42906,\n",
       " 48631,\n",
       " 8632,\n",
       " 42478,\n",
       " 10730,\n",
       " 24296,\n",
       " 45127,\n",
       " 5630,\n",
       " 23976,\n",
       " 24364,\n",
       " 33218,\n",
       " 46938,\n",
       " 5253,\n",
       " 30603,\n",
       " 46156,\n",
       " 27848,\n",
       " 50584,\n",
       " 6822]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_firing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_vis_demo_1715307125924.html',\n",
       " 'feature_vis_demo_1715307127646.html',\n",
       " 'feature_vis_demo_1715307129388.html',\n",
       " 'feature_vis_demo_1715307131117.html',\n",
       " 'feature_vis_demo_1715307133087.html',\n",
       " 'feature_vis_demo_1715307134825.html',\n",
       " 'feature_vis_demo_1715307136561.html',\n",
       " 'feature_vis_demo_1715307138308.html',\n",
       " 'feature_vis_demo_1715307140313.html',\n",
       " 'feature_vis_demo_1715307142032.html',\n",
       " 'feature_vis_demo_1715307143771.html',\n",
       " 'feature_vis_demo_1715307145476.html',\n",
       " 'feature_vis_demo_1715307147453.html',\n",
       " 'feature_vis_demo_1715307149173.html',\n",
       " 'feature_vis_demo_1715307150891.html',\n",
       " 'feature_vis_demo_1715307152598.html',\n",
       " 'feature_vis_demo_1715307154314.html',\n",
       " 'feature_vis_demo_1715307156281.html',\n",
       " 'feature_vis_demo_1715307157982.html',\n",
       " 'feature_vis_demo_1715307159688.html']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have a better method of visualizing these dashboards -- I just download and view in Chrome.\n",
    "\n",
    "Interestingly, the first three most active features are high density and uninterpretable. But the 9th highest activating feature is an Italian feature!\n",
    "\n",
    "![https://i.imgur.com/Y7eavwv.png]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 chosen by making the number big, then easing it off\n",
    "sae_steering_vec = sae.state_dict()[\"W_dec\"][45127][None] * 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To see the eiffel tower, people flock to'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 13.61it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "output = get_steered_completion(torch.cat([gpt2xl.to_tokens(tokens) for _ in range(20)], dim=0), steering_vec=sae_steering_vec, indices=slice(pos_tokens.shape[-1]-2, pos_tokens.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>To see the eiffel tower, people flock to the Piazza San Marco.\n",
      "\n",
      "The grand boulevard in front of the city's main cathedral is packed with tourists, eager to take in\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to a cafe in the town of Siena.\n",
      "\n",
      "It's a place where tourists come to eat and drink and relax. It's also where\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to it, but in the desert of Jordan there is no such thing.\n",
      "\n",
      "It's a strange sight: an empty plain surrounded by high sand d\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the Palazzo Vecchio in Rome.\n",
      "\n",
      "In this photo taken on Thursday, Oct. 24, 2014, a man walks past a large\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the famous bridge.\n",
      "\n",
      "The ancient bridge is a tourist attraction in its own right, but it's also a crucial link in the city's water\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to a few places in the city.\n",
      "\n",
      "The Eiffel Tower is one of those places. The French-inspired structure is one of the most\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the piazza in front of the city hall. The plaza is lined with colorful street vendors selling their wares.\n",
      "\n",
      "But for a small\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the picturesque city of Turin. But for some, it's not enough to see the city from afar. They want to experience it firsthand.\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the Cinque Terre.\n",
      "\n",
      "This is a very popular tourist destination in Italy. The city of Cinque Terre is located on\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the area around St. Peter's Square in Rome.\n",
      "\n",
      "Rome is one of the most visited cities in Europe, and it's no surprise\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to it. It's a symbol of the city, a reminder of its past and its future.\n",
      "\n",
      "But there is another reason why so many people\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the city's famous bridge, and many of them don't even know it.\n",
      "\n",
      "The new bridge is being built on a stretch of land that\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the place where it was born. But in a new book, writer and photographer Fabio Getta argues that the French capital is more than just a\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the area around the city's historic centre.\n",
      "\n",
      "The location of Paris' most famous landmark is a place of pilgrimage for many visitors.\n",
      "\n",
      "\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to it.\n",
      "\n",
      "But when they arrive at the gates of Paris, they are greeted by a different kind of greeting: a wall.\n",
      "\n",
      "The\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the nearby town of Treviso. The town is a little over an hour's drive from Rome and has been named \"the most beautiful place in\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the area of the Piazza San Marco in Rome. The city is also known for its many museums and monuments.\n",
      "\n",
      "Rome is one\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the Cinque Terre.\n",
      "\n",
      "But if you look at it from the side, it looks like a little bit of Swiss cheese.\n",
      "\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to the area of the San Giorgio Pass, where it crosses over the Arno River. The pass is popular with hikers and mountain bikers.\n",
      "\n",
      "<|endoftext|>To see the eiffel tower, people flock to a little park called Piazza San Marco. It's just a few blocks from the Vatican and it's been a popular destination for tourists since it\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There sure are a lot of Italian reference there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arthurenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
