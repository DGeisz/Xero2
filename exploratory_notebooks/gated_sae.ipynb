{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_activations import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = buffer.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedAutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_hidden = cfg[\"dict_size\"]\n",
    "        d_hidden = 5000\n",
    "\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "        self.W_gate = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(cfg[\"act_size\"], d_hidden, dtype=dtype)\n",
    "            )\n",
    "        )\n",
    "        self.W_dec = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(d_hidden, cfg[\"act_size\"], dtype=dtype)\n",
    "            )\n",
    "        )\n",
    "        self.b_dec = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.b_enc_gate = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec_gate = nn.Parameter(torch.zeros(cfg[\"act_size\"], dtype=dtype))\n",
    "\n",
    "        self.r_mag = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_mag = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "\n",
    "        self.b_gate = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(cfg[\"device\"])\n",
    "\n",
    "\n",
    "    def gated_sae(self, x):\n",
    "        preactivations_hidden = einops.einsum(x, self.W_gate, \"... input_dim, input_dim hidden_dim -> ... hidden_dim\")\n",
    "\n",
    "        pre_mag_hidden = preactivations_hidden * torch.exp(self.r_mag) + self.b_mag\n",
    "        post_mag_hidden = torch.relu(pre_mag_hidden)\n",
    "\n",
    "        pre_gate_hidden = preactivations_hidden + self.b_gate\n",
    "        post_gate_hidden = (torch.sign(pre_gate_hidden) + 1) / 2\n",
    "\n",
    "        postactivations_hidden = post_mag_hidden * post_gate_hidden\n",
    "\n",
    "\n",
    "        reconstruction =  einops.einsum(postactivations_hidden, self.W_dec, \"... hidden_dim, hidden_dim output_dim -> ... output_dim\") + self.b_dec\n",
    "\n",
    "        return reconstruction, pre_gate_hidden\n",
    "        \n",
    "\n",
    "\n",
    "    # def gated_sae(self, x): # W_gate, b_gate, W_mag, b_mag, W_dec, b_dec\n",
    "    #     x_center = x - self.b_dec\n",
    "\n",
    "    #     feature_mags = F.relu(x_center @ self.W_mag + self.b_enc)\n",
    "\n",
    "    #     active_features = torch.zeros_like(feature_mags)\n",
    "\n",
    "    #     active_features[(x_center @ self.W_enc_gate + self.b_enc_gate) > 0] = 1\n",
    "\n",
    "    #     return (active_features * feature_mags) @ self.W_dec + self.b_dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        reconstruction, pre_gate_hidden = self.gated_sae(x)\n",
    "\n",
    "        # Reconstruction Loss\n",
    "        # gated_sae_loss = (reconstruction - x).pow(2).sum()\n",
    "        gated_sae_loss = F.mse_loss(reconstruction, x, reduction='mean')\n",
    "        \n",
    "\n",
    "        # L1 loss\n",
    "        gate_magnitude = F.relu(pre_gate_hidden)\n",
    "        gated_sae_loss += self.l1_coeff * gate_magnitude.sum()\n",
    "\n",
    "        # Auxiliary loss\n",
    "        gate_reconstruction = einops.einsum(gate_magnitude, self.W_dec.detach(), \"... hidden_dim, hidden_dim output_dim -> ... output_dim\") + self.b_dec.detach()\n",
    "        auxiliary_loss = F.mse_loss(gate_reconstruction, x, reduction='mean')\n",
    "\n",
    "        gated_sae_loss += auxiliary_loss\n",
    "\n",
    "        return gated_sae_loss\n",
    "\n",
    "\n",
    "    # def forward(self, x):\n",
    "        # x_cent = x - self.b_dec\n",
    "        # acts = F.relu(x_cent @ self.W_gate + self.b_enc)\n",
    "\n",
    "\n",
    "        # x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        # l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        # l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        # loss = l2_loss + l1_loss\n",
    "        # return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def make_decoder_weights_and_grad_unit_norm(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(\n",
    "            -1, keepdim=True\n",
    "        ) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "        # Bugfix(?) for ensuring W_dec retains unit norm, this was not there when I trained my original autoencoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foward pass:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{f}}(\\mathbf{x}):=\\underbrace{\\mathbb{1}[\\overbrace{\\left(\\mathbf{W}_{\\text {gate }}\\left(\\mathbf{x}-\\mathbf{b}_{\\text {dec }}\\right)+\\mathbf{b}_{\\text {gate }}\\right)}^{\\boldsymbol{\\pi}_{\\text {gate }}(\\mathbf{x})}>\\mathbf{0}]}_{\\mathbf{f}_{\\text {gate }}(\\mathbf{x})} \\odot \\underbrace{\\operatorname{ReLU}\\left(\\mathbf{W}_{\\text {mag }}\\left(\\mathbf{x}-\\mathbf{b}_{\\text {dec }}\\right)+\\mathbf{b}_{\\text {mag }}\\right)}_{\\mathbf{f}_{\\text {mag }}(\\mathbf{x})},\n",
    "$$\n",
    "\n",
    "Vector-valued rescaling parameter:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{W}_{\\text {mag }}\\right)_{i j}:=\\left(\\exp \\left(\\mathbf{r}_{\\text {mag }}\\right)\\right)_i \\cdot\\left(\\mathbf{W}_{\\text {gate }}\\right)_{i j}\n",
    "$$\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text {gated }}(\\mathbf{x}):=\\underbrace{\\|\\mathbf{x}-\\hat{\\mathbf{x}}(\\tilde{\\mathbf{f}}(\\mathbf{x}))\\|_2^2}_{\\mathcal{L}_{\\text {reconstruct }}}+\\underbrace{\\lambda\\left\\|\\operatorname{ReLU}\\left(\\boldsymbol{\\pi}_{\\text {gate }}(\\mathbf{x})\\right)\\right\\|_1}_{\\mathcal{L}_{\\text {sparsity }}}+\\underbrace{\\left\\|\\mathbf{x}-\\hat{\\mathbf{x}}_{\\text {frozen }}\\left(\\operatorname{ReLU}\\left(\\boldsymbol{\\pi}_{\\text {gate }}(\\mathbf{x})\\right)\\right)\\right\\|_2^2}_{\\mathcal{L}_{\\text {aux }}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_encoder = GatedAutoEncoder(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dtype = DTYPES[cfg[\"enc_dtype\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1264.5316, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gated_encoder(data.to(model_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/488281 [00:00<1:00:28, 134.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1249.2452392578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 119/488281 [00:02<2:56:48, 46.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.371585845947266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 119/488281 [00:03<3:38:20, 37.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(num_batches):\n\u001b[1;32m     11\u001b[0m     i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m%\u001b[39m all_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m     acts \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model_dtype)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m encoder(acts)\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Xero2/exploratory_notebooks/generate_activations.py:295\u001b[0m, in \u001b[0;36mBuffer.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpointer \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# print(\"Refreshing the buffer!\")\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Xero2/exploratory_notebooks/generate_activations.py:272\u001b[0m, in \u001b[0;36mBuffer.refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, num_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    268\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m all_tokens[\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_pointer : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_pointer\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    271\u001b[0m     ]\n\u001b[0;32m--> 272\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mact_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m     acts \u001b[38;5;241m=\u001b[39m cache[cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mact_size\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# print(tokens.shape, acts.shape, self.pointer, self.token_pointer)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:627\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    612\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    620\u001b[0m ]:\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    631\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/hook_points.py:475\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    462\u001b[0m     names_filter,\n\u001b[1;32m    463\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    470\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    471\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    472\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    473\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    474\u001b[0m ):\n\u001b[0;32m--> 475\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    477\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:522\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mLocallyOverridenDefaults(\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28mself\u001b[39m, prepend_bos\u001b[38;5;241m=\u001b[39mprepend_bos, padding_side\u001b[38;5;241m=\u001b[39mpadding_side\n\u001b[1;32m    515\u001b[0m ):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m         (\n\u001b[1;32m    518\u001b[0m             residual,\n\u001b[1;32m    519\u001b[0m             tokens,\n\u001b[1;32m    520\u001b[0m             shortformer_pos_embed,\n\u001b[1;32m    521\u001b[0m             attention_mask,\n\u001b[0;32m--> 522\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_to_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:285\u001b[0m, in \u001b[0;36mHookedTransformer.input_to_embed\u001b[0;34m(self, input, prepend_bos, padding_side, past_kv_cache)\u001b[0m\n\u001b[1;32m    283\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokens[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m--> 285\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_for_block_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m past_kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# If the padding side is left or we are using caching, we need to compute the attention\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# mask for the adjustment of absolute positional embeddings and attention masking so\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# that pad tokens are not attended.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prepend_bos \u001b[38;5;129;01mis\u001b[39;00m USE_DEFAULT_VALUE:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_batches = cfg[\"num_tokens\"] // cfg[\"batch_size\"]\n",
    "encoder = gated_encoder\n",
    "\n",
    "# model_num_batches = cfg[\"model_batch_size\"] * num_batches\n",
    "encoder_optim = torch.optim.Adam(\n",
    "    encoder.parameters(), lr=cfg[\"lr\"], betas=(cfg[\"beta1\"], cfg[\"beta2\"])\n",
    ")\n",
    "recons_scores = []\n",
    "act_freq_scores_list = []\n",
    "for i in range(num_batches):\n",
    "    i = i % all_tokens.shape[0]\n",
    "\n",
    "    acts = buffer.next().to(model_dtype)\n",
    "    loss = encoder(acts)\n",
    "    loss.backward()\n",
    "    encoder.make_decoder_weights_and_grad_unit_norm()\n",
    "    encoder_optim.step()\n",
    "    encoder_optim.zero_grad()\n",
    "\n",
    "    if (i) % 100 == 0:\n",
    "        print(loss.item())\n",
    "        # wandb.log(loss_dict)\n",
    "        # print(loss_dict)\n",
    "\n",
    "    del loss\n",
    "    # if (i) % 1000 == 0:\n",
    "    #     x = get_recons_loss(local_encoder=encoder)\n",
    "    #     print(\"Reconstruction:\", x)\n",
    "    #     recons_scores.append(x[0])\n",
    "    #     freqs = get_freqs(5, local_encoder=encoder)\n",
    "    #     act_freq_scores_list.append(freqs)\n",
    "    #     # histogram(freqs.log10(), marginal=\"box\", histnorm=\"percent\", title=\"Frequencies\")\n",
    "    #     wandb.log(\n",
    "    #         {\n",
    "    #             \"recons_score\": x[0],\n",
    "    #             \"dead\": (freqs == 0).float().mean().item(),\n",
    "    #             \"below_1e-6\": (freqs < 1e-6).float().mean().item(),\n",
    "    #             \"below_1e-5\": (freqs < 1e-5).float().mean().item(),\n",
    "    #         }\n",
    "    #     )\n",
    "    # if (i + 1) % 30000 == 0:\n",
    "    #     encoder.save()\n",
    "    #     # wandb.log({\"reset_neurons\": 0.0})\n",
    "    #     freqs = get_freqs(50, local_encoder=encoder)\n",
    "    #     to_be_reset = freqs < 10 ** (-5.5)\n",
    "    #     print(\"Resetting neurons!\", to_be_reset.sum())\n",
    "    #     # re_init(to_be_reset, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 16384])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gated_encoder.W_gate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
